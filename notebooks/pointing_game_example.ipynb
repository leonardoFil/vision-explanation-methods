{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LFilipe\\OneDrive - WavEC Offshore Renewables\\Documents\\EUSCORES\\vision-explanation-methods\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import PIL.Image as Image\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from ml_wrappers.model.image_model_wrapper import PytorchDRiseWrapper\n",
    "\n",
    "from vision_explanation_methods.evaluation.pointing_game import PointingGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get path of an example image\n",
    "# BASE_DIR = \"../python/vision_explanation_methods/images/\"\n",
    "# img_fname = os.path.join(BASE_DIR, \"2.jpg\")\n",
    "img_fname = r\"C:\\Users\\LFilipe\\OneDrive - WavEC Offshore Renewables\\Documents\\GitHub\\dolphin_detection\\results_yolo11_inf\\frame_000011.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get fasterrcnn model\n",
    "detection_model = YOLO(r\"C:\\Users\\LFilipe\\OneDrive - WavEC Offshore Renewables\\Documents\\GitHub\\dolphin_detection\\model\\M5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 731.8ms\n",
      "Speed: 0.5ms preprocess, 731.8ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 613.5ms\n",
      "Speed: 0.0ms preprocess, 613.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "max(): Expected reduction dim 1 to have non-zero size.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#find saliency scores for top 20% of salient pixels\u001b[39;00m\n\u001b[32m      2\u001b[39m pg = PointingGame(detection_model)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m salient_scores = \u001b[43mpg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpointing_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_fname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m.8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LFilipe\\OneDrive - WavEC Offshore Renewables\\Documents\\EUSCORES\\vision-explanation-methods\\venv\\Lib\\site-packages\\vision_explanation_methods\\evaluation\\pointing_game.py:101\u001b[39m, in \u001b[36mPointingGame.pointing_game\u001b[39m\u001b[34m(self, imagelocation, index, threshold, num_masks)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     99\u001b[39m     detections = \u001b[38;5;28mself\u001b[39m._model.predict(img_input)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m saliency_scores = \u001b[43mdrise\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDRISE_saliency\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Repeated the tensor to test batching\u001b[39;49;00m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_detections\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# This is how many masks to run -\u001b[39;49;00m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# more is slower but gives higher quality mask.\u001b[39;49;00m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnumber_of_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask_padding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# This is the resolution of the random masks.\u001b[39;49;00m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# High resolutions will give finer masks, but more need to be\u001b[39;49;00m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# run.\u001b[39;49;00m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask_res\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Turns progress bar on/off.\u001b[39;49;00m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m temp = saliency_scores[\u001b[32m0\u001b[39m][index][\u001b[33m'\u001b[39m\u001b[33mdetection\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    119\u001b[39m temp[temp < threshold] = -\u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LFilipe\\OneDrive - WavEC Offshore Renewables\\Documents\\EUSCORES\\vision-explanation-methods\\venv\\Lib\\site-packages\\vision_explanation_methods\\explanations\\drise.py:262\u001b[39m, in \u001b[36mDRISE_saliency\u001b[39m\u001b[34m(model, image_tensor, target_detections, number_of_masks, mask_res, mask_padding, device, verbose)\u001b[39m\n\u001b[32m    257\u001b[39m     affinity_scores = []\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m (target_detection, masked_detection) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target_detections,\n\u001b[32m    260\u001b[39m                                                     masked_detections):\n\u001b[32m    261\u001b[39m         affinity_scores.append(\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m             \u001b[43mcompute_affinity_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_detection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mmasked_detection\u001b[49m\u001b[43m)\u001b[49m.detach().to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    264\u001b[39m     mask_records.append(MaskAffinityRecord(\n\u001b[32m    265\u001b[39m         mask=mask.to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    266\u001b[39m         affinity_scores=affinity_scores)\n\u001b[32m    267\u001b[39m     )\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m saliency_fusion(mask_records, device, verbose=verbose)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LFilipe\\OneDrive - WavEC Offshore Renewables\\Documents\\EUSCORES\\vision-explanation-methods\\venv\\Lib\\site-packages\\vision_explanation_methods\\explanations\\drise.py:141\u001b[39m, in \u001b[36mcompute_affinity_scores\u001b[39m\u001b[34m(base_detections, masked_detections)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute highest affinity score between two sets of detections.\u001b[39;00m\n\u001b[32m    132\u001b[39m \n\u001b[32m    133\u001b[39m \u001b[33;03m:param base_detections: Set of detections to get affinity scores for\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m \u001b[33;03m:rtype: Tensor of shape D, where D is number of base detections\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    140\u001b[39m score_matrix = compute_affinity_matrix(base_detections, masked_detections)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mIndexError\u001b[39m: max(): Expected reduction dim 1 to have non-zero size."
     ]
    }
   ],
   "source": [
    "#find saliency scores for top 20% of salient pixels\n",
    "pg = PointingGame(detection_model)\n",
    "salient_scores = pg.pointing_game(img_fname, 0, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the saliency map for highly salient pixels\n",
    "test_image = Image.open(img_fname).convert('RGB')\n",
    "figure = pg.visualize_highly_salient_pixels(test_image, salient_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find what percent of salient pixels overlap with the area of the ground truth bbox\n",
    "# each channel of the saliency scores matrix should be the same, which is why you can\n",
    "# just take the first index of s\n",
    "overlap = pg.calculate_gt_salient_pixel_overlap(salient_scores, [247, 192, 355, 493])\n",
    "overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
